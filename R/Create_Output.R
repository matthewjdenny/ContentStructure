#' A Function to generate diagnostic plots and interpretable output for a list of 1 to N model output objects from running the ContentStructure model.
#' 
#' @param data_name The name of organization data file as in the function to run the model. Defaults to NULL if we are not saving any output to disk or reading in any intermediate data from disk.
#' @param only_generate_summaries If TRUE, then only generate a one-page-per-cluster pdf summary of model output for each county, otherwise generate a ton of output.
#' @param estimation_output_directory The directory where all .Rdata files generated by Run_Full_Model() are stored. Defaults to NULL if we are not reading in any intermediate data from disk. 
#' @param raw_data_directory The directory where all .Rdata files contianing the raw data and vocabulary files. Defaults to NULL if we are not reading in any intermediate data from disk. 
#' @param print_agg_stats If TRUE, generates a plot comapring topics frequency across all clusters and a trace plot of th topic model log likelihood -- very useful.
#' @param using_county_email_data Logical if you are using North Carolina County Government email data that are properly formatted to produce aggregate level output. 
#' @param Topic_Model_Burnin The number of iterations of Gibbs sampling that should be discarded before calculated Geweke statistic to determine model convergence. You will simply want to set it pretty low and then look at the trace to determine where you should set it to provide evidence of convergence.
#' @param Thin The number of iterations to skip in the MH for LSM chain when generating output. Set to 1 as default does not thin but can be set higher to make plotting easier if you took a lot of samples.
#' @param Skip The number of MH for LSM iterations to skip when generating out (if your burnin was not long enough).
#' @param Used_MP If TRUE then will generate output for mixing parameter estimates (if they were used).
#' @param MP_Name If we are using mixing parameters then specify the name of the binary variable for which mixing parameter estimates were created in the author_attributes dataset.
#' @param Auth_Attr The author attributes dataframe.
#' @param Vocabulary The vocabulary dataframe.
#' @param raw_data_list A vector containing the names of organization data files as in the function to run the model. Should only contain names of those organizations for which output has already been created. Only for use with county government datasets (not for public use).
#' @param estimation_output_list A vector containing the names of organization data files as in the function to run the model. Should only contain names of those organizations for which output has already been created. Only for use with county government datasets (not for public use).
#' @param load_results_from_file A logical which defaults to FALSE. If TRUE, then the function will load data named by data_name output from Run_Full_Model (note that save_results_to_file must be set to TRUE in this function) in the data_directory and use this to generate output. 
#' @param Estimation_Results A list object returned by Run_Full_Model that will be used to generate output. Can be NULL if load_results_from_file == TRUE in which case intermediate results saved to disk will be used instead. 
#' @param save_results  Defaults to FALSE, if TRUE, then data_name and data_directory must be supplied and .pdfs will be created for all plots. 
#' @param output_directory A directory where we wish to save the output of this function.
#' @param model_output_list If you prefer to provide a list object where each entry is the object returned by the Run_Full_Model() function. This is a way to circumvent reading in files from disk if we want to keep all information in our current R session.
#' @param pretty_output_names An optional (vector) of file names that can be used to name the output .pdf files generated by this function. Useful if you would like your output files to look like Org_Plot_X.pdf instead of Org_Results_of_9-30-15_Plot_X.pdf, for example. 
#' @param clean_up_intermediate_files Only set to TRUE if you are using full automation with file saving and multiple counties, not for public use. 
#' @return A list object with 4 entries and the following structure: Cluster_Data contains cluster level data including top words and mixing parameters (with standard errors) if applicable. Actor_Data contains actors level data (all of the Auth_Attr dataframe) plus average latent positions for each actor in each dimension (2 currently), for each cluster. Token_Data contains the counts of each token for each topic, along with the edge counts for that topic and the cluster assignment for it. Vocabulary simply holds the vocabulary as a vector for easy handling.
#' @export
Create_Output <- function(data_name = NULL,
                          only_generate_summaries = T, 
                          estimation_output_directory = NULL,
                          raw_data_directory = NULL,
                          print_agg_stats = T,
                          using_county_email_data = F,
                          Topic_Model_Burnin = 50,
                          Skip = 0, 
                          Thin = 1,
                          Used_MP = F,
                          MP_Name = "Gender",
                          raw_data_list = NULL,
                          estimation_output_list = NULL,
                          Auth_Attr = NULL,
                          Vocabulary = NULL,
                          load_results_from_file = F,
                          Estimation_Results = NULL,
                          save_results = F,
                          output_directory = NULL,
                          model_output_list = NULL,
                          pretty_output_names = NULL,
                          clean_up_intermediate_files = FALSE
                          ){
  
  if(save_results){
    substrRight <- function(x, n){
      substr(x, nchar(x)-n+1, nchar(x))
    }
    
    lastchar <- substrRight(model_output_directory,1)
    
    if(lastchar == "/"){
      #we are all set
    }else{
      #add a trailing slash to we save to right place
      model_output_directory <- paste(model_output_directory,"/",sep = "")
    }
    
    lastchar2 <- substrRight(output_directory,1)
    
    if(lastchar2 == "/"){
      #we are all set
    }else{
      #add a trailing slash to we save to right place
      output_directory <- paste(output_directory,"/",sep = "")
    }
    
    #if no pretty output names were provided
    if(is.null(pretty_output_names)){
      if(using_county_email_data){
        pretty_output_names <- estimation_output_list
      }else{
        pretty_output_names <- data_name
      }
    }
    
    setwd(model_output_directory)
  }
  #loop over all counties in list:
  if(using_county_email_data){
    for(i in 1:length(raw_data_list)){
      
      #load data
      if(load_results_from_file){
        cat("Current County:",raw_data_list[i],"--",i,"of",length(raw_data_list), " \n")
        load( paste(model_output_directory,raw_data_list[i],".Rdata", sep = ""))
        load( paste(model_output_directory,estimation_output_list[i],".Rdata", sep = ""))
        
        #specific to our data, may need to change
        Auth_Attr <- author_attributes
        Vocabulary <- data.frame(vocabulary,stringsAsFactors = F)
        Estimation_Results <- Output
      }else{
        Estimation_Results <- model_output_list[[i]]
      }

      temp <- Generate_Model_Diagnsotics(
                LS_Actor = 8, 
                out_directory = output_directory, 
                vocab = Vocabulary, 
                output_name = pretty_output_names[i],
                Thin_Itterations = Thin,
                skip_first = Skip,
                Author_Attributes = Auth_Attr,
                proportion_in_confidence_contour  = 0.9, 
                topic_model_burnin = Topic_Model_Burnin, 
                pretty_name =  pretty_output_names,
                only_print_summaries = only_generate_summaries,
                print_agregate_level_stats = print_agg_stats, 
                used_county_email_data = using_county_email_data,
                used_binary_mixing_attribute = Used_MP, 
                binary_mixing_attribute_name = MP_Name,
                Estimation_Results = Estimation_Results,
                save_results = save_results
      )
      
      #if we are on the first iteration, make the return object equal to temp, oterwise, append
      if(i == 1){
        county_data <- list(temp)
        word_mat <- temp[[3]]
        metadata <- word_mat[,1:3]
        len <- length(word_mat[1,])
        nr <-nrow(word_mat[,14:len])
        nc <-ncol(word_mat[,14:len])
        word_mat <- matrix(as.numeric(word_mat[,14:len]),nrow = nr, ncol= nc)
        all_vocab <- temp[[4]]
        all_words <- slam::as.simple_triplet_matrix(word_mat)
      }else{
        county_data <- append(county_data,list(temp))
        word_mat <- temp[[3]]
        metadata <- rbind(metadata,word_mat[,1:3])
        len <- length(word_mat[1,])
        nr <-nrow(word_mat[,14:len])
        nc <-ncol(word_mat[,14:len])
        word_mat <- matrix(as.numeric(word_mat[,14:len]),nrow = nr, ncol= nc)
        
        cur_vocab <- temp[[4]]
        new_vocab <- unique(rbind(all_vocab,cur_vocab))
        #the new words that the current vocab is adding
        addition <- new_vocab[-(1:length(all_vocab[,1])),]
        #create a block of zeros to append to the right side of the existing matrix (becasue those documents do not use the new words)
        add_word_matrix <- slam::simple_triplet_zero_matrix(nrow =nrow(all_words),ncol= length(addition))
        #stick them together
        all_words <- cbind(all_words,add_word_matrix)
        
        #now create a new matrix with all the right row indicies to 
        current_word_matrix <- slam::simple_triplet_zero_matrix(nrow = nrow(word_mat),ncol= length(new_vocab[,1]))
        for(j in 1:length(cur_vocab[,1])){
          index <- which(new_vocab[,1] == cur_vocab[j,1])
          current_word_matrix[,index] <- word_mat[,j]
        }
        
        #update
        all_words <- rbind(all_words,current_word_matrix)
        all_vocab <- new_vocab
        tv <- unlist(all_vocab[,1])
        colnames(all_words) <- tv
      }
      
    }#end of loop
  }else{
    if(!is.null(Estimation_Results$author_attributes)){
      Vocabulary <- Estimation_Results$vocabulary
      Auth_Attr <- Estimation_Results$author_attributes
    }
    #if we are not using the county email dataset we can only go one at a time
    toreturn <- Generate_Model_Diagnsotics(
                   LS_Actor = 8, 
                   out_directory = output_directory, 
                   vocab = Vocabulary, 
                   output_name =  pretty_output_names,
                   Thin_Itterations = Thin,
                   skip_first = Skip,
                   Author_Attributes = Auth_Attr,
                   proportion_in_confidence_contour  = 0.9, 
                   topic_model_burnin = Topic_Model_Burnin, 
                   pretty_name =  pretty_output_names,
                   only_print_summaries = only_generate_summaries,
                   print_agregate_level_stats = print_agg_stats, 
                   used_county_email_data = using_county_email_data,
                   used_binary_mixing_attribute = Used_MP, 
                   binary_mixing_attribute_name = MP_Name,
                   Estimation_Results = Estimation_Results,
                   save_results = save_results
                   )
  
  # now we clean up the intermediate datasets and save everything
    if(clean_up_intermediate_files){
      load(paste("Sample_",data_name,".Rdata",sep = ""))
      p1 <- pipe(paste("rm Sample_",data_name,".Rdata",sep = ""),"r")
      close(p1)
      p2 <- pipe(paste("rm Model_Output_",data_name,".Rdata",sep = ""), "r")
      close(p2)
      Return_List <- Return_List[-c(5,8)]
      save(Return_List, file = paste("MCMC_Output_",data_name,".Rdata",sep = ""))
    }
  }
  
  if(using_county_email_data){
    token_master_data <- list(tv,all_words,metadata)
    toreturn <- list(token_master_data,county_data)
  }else{
    cat("Returning Dataset \n")
  }
    
  return(toreturn)
}